{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-accel-recog.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/monstrim/tf-accel-recog/blob/master/TF_accel_recog.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "m-CxIF66xSWm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TF-accel-recog\n",
        "\n",
        "### Goal\n",
        "* To label a time series of 6 channels of motion capturing sensors (XYZ accel + gyro) according to activity performed (walking, sitting, steps up, etc). The model we're building here should be then able to learn a similar task, but involving skate tricks rather than walking/sitting.\n",
        "\n",
        "### Dataset\n",
        "* Description: http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones\n",
        "* Download link: http://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip"
      ]
    },
    {
      "metadata": {
        "id": "J6eb4DMxqlxI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Framework setup"
      ]
    },
    {
      "metadata": {
        "id": "LpfEKxpkSFkQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OVFEywmgSAsM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from zipfile import ZipFile\n",
        "from urllib import request\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.eager as tfe #hopefuly this won't be needed for long\n",
        "\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0QSRdeltShk3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset_url = (\n",
        "    'http://archive.ics.uci.edu/ml/'\n",
        "    + 'machine-learning-databases/'\n",
        "    + '00240/UCI%20HAR%20Dataset.zip'\n",
        ")\n",
        "filename = 'HAR_dataset.zip'\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    request.urlretrieve(dataset_url, filename)\n",
        "\n",
        "with ZipFile(filename) as archive:\n",
        "    files = [file for file in archive.namelist() \n",
        "             if not file.startswith('__MACOS')]\n",
        "    archive.extractall(path='data', members=files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K9sg4MWSryiH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Getting acquainted with the data\n",
        "\n",
        "Let's begin by checking the files we downloaded."
      ]
    },
    {
      "metadata": {
        "id": "sqQecLF4rFTU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[os.path.join(dp, f) for dp, dn, fn in os.walk('data') for f in fn]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zgCdT5Nnu2yV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It seems from the README.txt (which we get an error when we try to open here, but _trust me I checked_), that the X_train files contain only the preprocessed values (_a whole 500+ columns of it!_), and those aren't interesting for our needs. We want the raw data, and that's in the Inertial Signals folder. Thing is, it's one file per channel, and it's batched/windowed, so we need to look at the files and see how we can tease out the raw data as a time series, one line per sample, one column per channel."
      ]
    },
    {
      "metadata": {
        "id": "zgbYErfxrNHe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "samplefile = (\n",
        "    'data/UCI HAR Dataset/train/'\n",
        "    + 'Inertial Signals/body_gyro_x_train.txt'\n",
        ")\n",
        "\n",
        "with open(samplefile) as file:\n",
        "    for i in range(10):\n",
        "        line = file.readline().rstrip().split()\n",
        "        print(i)\n",
        "        print(line[0:64])\n",
        "        print(line[64:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DLcteStiuP7i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "OK. Each line has 128 samples, at 50Hz, with 50% overlap, so the first 64 samples on each line are the same as the last 64 on the previous one. To get one long sample, we need to select the first 64 samples on each line, then concatenate lines (checking the subject_train.txt file to see if we're still in the same run?). That will become a time series for a single channel, at 50Hz. We then need to concatenate the result for different files to get a single 6-channel time series.\n",
        "\n",
        "However, we must remember the original preprocessing (and the labeling) was performed on the 128-sample windows (64 if we disregard the overlap), so each line on the train and test files will correspond to 64 lines in our raw time series."
      ]
    },
    {
      "metadata": {
        "id": "DpcKBn13Qj-q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dataset pipeline test"
      ]
    },
    {
      "metadata": {
        "id": "Q7_5H3-Hzj2T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_train_dir = 'data/UCI HAR Dataset/train/Inertial Signals/'\n",
        "raw_train_files = [\n",
        "    os.path.join(raw_train_dir, x)\n",
        "    for x in [\n",
        "        'total_acc_x_train.txt',\n",
        "        'total_acc_y_train.txt',\n",
        "        'total_acc_z_train.txt',\n",
        "        'body_gyro_x_train.txt',\n",
        "        'body_gyro_y_train.txt',\n",
        "        'body_gyro_z_train.txt'\n",
        "    ]\n",
        "]\n",
        "\n",
        "raw_train_files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cd9i8BQ53cZp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = (\n",
        "    tf.data.Dataset\n",
        "    .from_tensor_slices(raw_train_files)\n",
        "    .interleave(\n",
        "        lambda x: (\n",
        "            #tf.data.Dataset.from_tensors(x).repeat(10)\n",
        "            tf.data.TextLineDataset(x)\n",
        "        ), 6\n",
        "    )\n",
        "    #.map(lambda x: tf.decode_csv(x, [[0]], field_delim=' '))\n",
        "    #.batch(6)\n",
        "    .take(10)\n",
        ")\n",
        "for item in tfe.Iterator(dataset):\n",
        "    print(item)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7dujQG4JHBu7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "WHY THE FUCK DOESN'T THIS ASSHOLE SEPARATE VALUES WITH A FUCKING COMMA??? Who uses fucking fixed width columns?? Python's string split function works just fine, but we're supposed to apply only tf functions inside a dataset map, plus split returns a list not a Tensor. SERIOUSLY, WTF.\n",
        "\n",
        "We've tried:\n",
        "* tf.decode_raw: will return one uint8 per character. We'd have to group them in 16-long batches, convert _back_ to characters, concatenate, then parse as a float. And I don't think there's a function just for parsing text values.\n",
        "* tf.split_string: it needs a different shape? Can't figure this shit out, the error messages are (surprise?) _not fucking helpful_.\n",
        "* tf.expand_dims + tf.split_string: Let's try adding the dimension it needs then? We can't because... hell if I know. The only thing the error message tells me it it's got something to do with being run in eager mode.\n",
        "\n",
        "I've no idea what to do with this shit. My best guess is, abandon the Tensorflow dataset pipeline, make a preprocessing step in numpy or some shit, save the preprocessed file back to disk and work on it instead.\n",
        "\n",
        "God fucking damn it."
      ]
    },
    {
      "metadata": {
        "id": "im5JJusIUpOg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Made peace with the fact we're gonna have to do this as an actual pre-step and save a processed file, rather than do it live in the Dataset pipeline.\n",
        "\n",
        "### Goal\n",
        "To preprocess 6 files into a single one. Each of the raw files has 128 samples per line, in fixed-width format. We need to get the first 64 of these, then transpose it, then concatenate all the lines as one long vector. That will be one column of the final file, and we need 6 of those.\n",
        "\n",
        "Coffee machine hasn't been refilled in 4 hours tho, so I AIN'T GONNA DO THIS SHIT NOW."
      ]
    },
    {
      "metadata": {
        "id": "5Tid0k4aWdpw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}